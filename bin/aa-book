#!/bin/bash
# aa-book: Anna's Archive â†’ NAS book pipeline
# Usage: aa-book <command> [args...]
# Requires: agent-secrets (annas_archive_key), agent-browser, curl, python3

set -e

COOKIE_FILE="${HOME}/.config/annas-archive/cookies.txt"
SESSION_FILE="${HOME}/.config/annas-archive/aa-session.json"
CONFIG_DIR="${HOME}/.config/annas-archive"
BASE_URL="https://annas-archive.li"
LOCAL_STAGING="/tmp/aa-book"
NAS_HOST="joel@three-body"
NAS_BOOKS_DIR="/volume1/home/joel/books"
PROGRESS_LOG="/tmp/aa-book-progress.log"

mkdir -p "$CONFIG_DIR" "$LOCAL_STAGING"

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
CYAN='\033[0;36m'
NC='\033[0m'

log() { echo -e "${GREEN}[$(date +%H:%M:%S)]${NC} $1" | tee -a "$PROGRESS_LOG"; }
warn() { echo -e "${YELLOW}[$(date +%H:%M:%S)]${NC} $1" | tee -a "$PROGRESS_LOG"; }
error() { echo -e "${RED}[$(date +%H:%M:%S)]${NC} $1" | tee -a "$PROGRESS_LOG"; }
status() { echo -e "${CYAN}[STATUS]${NC} $1" | tee -a "$PROGRESS_LOG"; }

# --- Auth ---
# Browser-based login (required for fast downloads)
browser_login() {
    local secret
    if command -v secrets &> /dev/null; then
        secret=$(secrets lease annas_archive_key --ttl 15m --raw 2>/dev/null)
    fi
    [[ -z "$secret" ]] && { error "Failed to lease annas_archive_key from agent-secrets"; return 1; }

    log "Logging in via browser..."
    agent-browser open "${BASE_URL}/account/" 2>/dev/null
    agent-browser wait --load networkidle 2>/dev/null

    # Fill secret key and submit. Support both legacy "@e123" and current "ref=e123" snapshot formats.
    local snapshot
    snapshot="$(agent-browser snapshot -i 2>/dev/null || true)"
    echo "$snapshot" | grep -q 'Secret key' || { error "Login page not found"; return 1; }

    local key_ref
    key_ref="$(echo "$snapshot" | grep 'Secret key' | grep -m1 -oE 'ref=e[0-9]+' | cut -d= -f2)"
    if [[ -z "$key_ref" ]]; then
        key_ref="$(echo "$snapshot" | grep 'Secret key' | grep -m1 -oE '@e[0-9]+' | sed 's/@//')"
    fi

    local btn_ref
    btn_ref="$(echo "$snapshot" | grep 'button \"Log in\"' | grep -m1 -oE 'ref=e[0-9]+' | cut -d= -f2)"
    if [[ -z "$btn_ref" ]]; then
        btn_ref="$(echo "$snapshot" | grep 'button \"Log in\"' | grep -m1 -oE '@e[0-9]+' | sed 's/@//')"
    fi

    [[ -z "$key_ref" || -z "$btn_ref" ]] && { error "Could not resolve login form references"; return 1; }

    agent-browser fill "$key_ref" "$secret" 2>/dev/null
    agent-browser click "$btn_ref" 2>/dev/null
    agent-browser wait --load networkidle 2>/dev/null

    # Save session for reuse
    agent-browser state save "$SESSION_FILE" 2>/dev/null
    log "Login successful, session saved"
}

# Restore browser session or login fresh
ensure_browser_auth() {
    if [[ -f "$SESSION_FILE" ]]; then
        # state load fails when a browser is already running
        agent-browser close >/dev/null 2>&1 || true
        if agent-browser state load "$SESSION_FILE" 2>/dev/null; then
            # Verify still logged in
            agent-browser open "${BASE_URL}/account/" 2>/dev/null || true
            agent-browser wait --load networkidle 2>/dev/null || true
            local logged_in
            logged_in=$(agent-browser eval 'document.documentElement.classList.contains("aa-logged-in")' 2>/dev/null | tr -d '"' || true)
            if [[ "$logged_in" == "true" ]]; then
                log "Browser session restored"
                return 0
            fi
        fi
    fi
    browser_login
}

# curl-based login (for search only)
curl_login() {
    local secret
    if command -v secrets &> /dev/null; then
        secret=$(secrets lease annas_archive_key --ttl 15m --raw 2>/dev/null)
    fi
    [[ -z "$secret" ]] && { error "Failed to lease annas_archive_key from agent-secrets"; return 1; }
    log "Logging in via curl..."
    curl -s -c "$COOKIE_FILE" -b "$COOKIE_FILE" -L -X POST \
        -d "key=${secret}" "${BASE_URL}/account/" > /dev/null
    log "Login successful"
}

check_curl_login() {
    [[ -f "$COOKIE_FILE" ]] && curl -s -o /dev/null -w '%{http_code}' -b "$COOKIE_FILE" "${BASE_URL}/account/" | grep -q "200"
}

# --- Search ---
search_books() {
    local query="$1"
    local format_filter="${2:-}"

    log "Searching: $query"
    [[ -n "$format_filter" ]] && log "Format filter: $format_filter"

    local html=$(curl -s -b "$COOKIE_FILE" "${BASE_URL}/search?q=$(echo "$query" | sed 's/ /+/g')")

    echo "$html" | python3 -c "
import sys, re
from html import unescape

html = sys.stdin.read()
format_filter = '$format_filter'.lower()

pattern = r'href=\"/md5/([a-f0-9]{32})\"[^>]*>([^<]+)</a>.*?(\d+(?:\.\d+)?\s*[KMG]?B).*?(pdf|epub|mobi|djvu|azw3)'
matches = re.findall(pattern, html, re.IGNORECASE | re.DOTALL)

seen = set()
results = []
for md5, title, size, fmt in matches:
    if md5 in seen: continue
    seen.add(md5)
    title = unescape(title.strip())[:60]
    fmt = fmt.lower()
    if format_filter and fmt != format_filter: continue
    color = '\033[32m' if fmt == 'pdf' else '\033[33m'
    results.append((md5, title, size, fmt, f'{color}{fmt}\033[0m'))

results.sort(key=lambda x: (0 if x[3] == 'pdf' else 1, x[3]))
for md5, title, size, fmt, disp in results[:15]:
    print(f'{md5}  {disp:12} {size:>8}  {title}')
" 2>/dev/null || {
        echo "$html" | grep -oP 'href="/md5/[a-f0-9]{32}"' | \
            sed 's/href="\/md5\///; s/"$//' | head -10 | sort -u
    }

    echo ""
    echo "Usage: aa-book download <md5>"
}

# --- Download ---
# Uses agent-browser to navigate the fast_download?no_redirect=1 page
# and extract the CDN URL, then downloads with curl and SCPs to NAS.
download_book() {
    local md5="$1"
    local output_dir="${2:-$LOCAL_STAGING}"
    local max_retries=3

    status "Downloading md5: $md5"

    # Track daily downloads (25/day limit)
    local today=$(date +%Y-%m-%d)
    local count_file="${CONFIG_DIR}/download_count_${today}.txt"
    local count=$(cat "$count_file" 2>/dev/null || echo 0)

    if [[ $count -ge 25 ]]; then
        error "Daily download limit reached (25/day). Try tomorrow."
        return 1
    fi

    # Ensure browser is authenticated
    ensure_browser_auth

    # Try each fast partner server via no_redirect page
    local download_url=""
    for server in 0 1 2 3 4 5; do
        status "Trying Fast Partner Server #$((server + 1))..."
        agent-browser open "${BASE_URL}/fast_download/${md5}/0/${server}?no_redirect=1" 2>/dev/null
        agent-browser wait --load networkidle 2>/dev/null

        # Extract CDN URL â€” look for links to partner CDN domains
        download_url=$(agent-browser eval 'Array.from(document.querySelectorAll("a[href]")).map(a=>a.href).find(h=>/\/(d[0-9]|get)\//.test(h)||/\.(pdf|epub|mobi)$/.test(h))||""' 2>/dev/null | tr -d '"')

        if [[ -n "$download_url" && "$download_url" != "" ]]; then
            log "Got CDN URL from server #$((server + 1))"
            break
        fi
        sleep 1
    done

    if [[ -z "$download_url" ]]; then
        error "Could not get download URL from any server"
        return 1
    fi

    # Derive filename
    local decoded_filename=$(python3 -c "import urllib.parse,sys; print(urllib.parse.unquote(sys.argv[1].split('/')[-1]))" "$download_url")
    local extension="${decoded_filename##*.}"
    local slug=$(echo "$decoded_filename" | sed 's/ -- .*//; s/[^a-zA-Z0-9 ]//g; s/  */ /g' | cut -c1-80 | sed 's/ /-/g' | tr '[:upper:]' '[:lower:]')
    local filename="${slug}.${extension}"
    local filepath="${output_dir}/${filename}"

    log "Downloading: $decoded_filename"
    log "Saving as: $filename"

    for attempt in $(seq 1 $max_retries); do
        if curl -L -o "$filepath" "$download_url" --connect-timeout 15 --max-time 300 --progress-bar 2>&1; then
            if [[ -f "$filepath" && $(stat -f%z "$filepath" 2>/dev/null || echo 0) -gt 1000 ]]; then
                echo $((count + 1)) > "$count_file"
                log "Download count today: $((count + 1))/25"
                break
            fi
        fi
        warn "Download attempt $attempt failed, retrying..."
        rm -f "$filepath" 2>/dev/null
        sleep 3
    done

    if [[ ! -f "$filepath" ]]; then
        error "Download failed after $max_retries attempts"
        return 1
    fi

    # Convert epub/mobi to pdf if Calibre available
    if [[ "$extension" == "epub" || "$extension" == "mobi" ]] && command -v ebook-convert &> /dev/null; then
        local pdf_path="${filepath%.*}.pdf"
        status "Converting $extension to PDF..."
        if timeout 120 ebook-convert "$filepath" "$pdf_path" --pdf-page-numbers 2>&1 | tail -3; then
            [[ -f "$pdf_path" ]] && { rm "$filepath"; filepath="$pdf_path"; log "Converted to PDF"; }
        else
            warn "Conversion failed, keeping original"
        fi
    fi

    # Transfer to NAS
    local year=$(date +%Y)
    local nas_dir="${NAS_BOOKS_DIR}/${year}"
    status "Transferring to NAS: ${NAS_HOST}:${nas_dir}/"
    ssh "$NAS_HOST" "mkdir -p '${nas_dir}'" 2>/dev/null
    if scp "$filepath" "${NAS_HOST}:${nas_dir}/$(basename "$filepath")" 2>&1; then
        log "Stored on NAS: ${nas_dir}/$(basename "$filepath")"
    else
        warn "NAS transfer failed â€” file kept at $filepath"
    fi

    echo "$filepath"
}

# --- Status ---
check_status() {
    local pid_file="${CONFIG_DIR}/current_job.pid"

    if [[ -f "$pid_file" ]]; then
        local pid=$(cat "$pid_file")
        if ps -p "$pid" > /dev/null 2>&1; then
            echo "ðŸ”„ Job running (PID: $pid)"
            echo ""
            echo "Recent progress:"
            tail -10 "$PROGRESS_LOG"
        else
            echo "âœ… Job completed"
            echo ""
            tail -20 "$PROGRESS_LOG" | grep -E '(STATUS|Complete|Error|complete)'
            rm "$pid_file"
        fi
    else
        echo "No active job"
        [[ -f "$PROGRESS_LOG" ]] && echo "Last log: tail -f $PROGRESS_LOG"
    fi

    local today=$(date +%Y-%m-%d)
    local count=$(cat "${CONFIG_DIR}/download_count_${today}.txt" 2>/dev/null || echo 0)
    echo ""
    echo "ðŸ“Š Downloads today: $count/25"
}

# --- Help ---
show_help() {
    echo "aa-book â€” Anna's Archive â†’ NAS book pipeline"
    echo ""
    echo "Commands:"
    echo "  search <query> [format]     Search (format: pdf, epub, mobi)"
    echo "  download <md5> [dir]        Download â†’ convert â†’ SCP to NAS"
    echo "  status                      Check job & daily quota"
    echo "  login                       Force browser re-login"
    echo ""
    echo "Examples:"
    echo "  aa-book search 'designing data intensive' pdf"
    echo "  aa-book download eaa2571df81dc9fa85cafaa89db7e81a"
    echo "  aa-book status"
    echo ""
    echo "Auth: agent-secrets (annas_archive_key) + agent-browser"
    echo "Storage: ${NAS_HOST}:${NAS_BOOKS_DIR}/YYYY/"
    echo "Daily limit: 25 fast downloads"
}

# --- Main ---
case "${1:-}" in
    search)
        shift
        check_curl_login || curl_login
        if [[ "$#" -ge 2 && "${!#}" =~ ^(pdf|epub|mobi)$ ]]; then
            format="${!#}"
            query="${*:1:$#-1}"
            search_books "$query" "$format"
        else
            search_books "$*"
        fi
        ;;
    download) download_book "$2" "${3:-}" ;;
    status) check_status ;;
    login) browser_login ;;
    help|--help|-h) show_help ;;
    *) show_help ;;
esac
